<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.6"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>RGB-D SLAM: RGB-D-SLAM</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">RGB-D SLAM
   </div>
   <div id="projectbrief">A SLAM implementation based on shape primitive extraction, based on RGB-D images</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.6 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

</div><!-- top -->
<div><div class="header">
  <div class="headertitle"><div class="title">RGB-D-SLAM </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>My take on a SLAM, based on shape primitive recognition with a RGB-D camera. The system extracts primitive shapes for the depth image, and use those primitives with 3D points and lines to estimate the observer position in space. A map is created with those points, lines and primitive shapes. See the Doxygen documentation for this program on <a href="https://baptistehudyma.github.io/RGB-D-SLAM/html/index.html">GitHub Pages</a>.</p>
<p>Each map feature is tracked using independent Kalman filter instead of bundle adjustment.</p>
<p>Point detection is based on <a href="https://ieeexplore.ieee.org/document/6126544">ORB detection and descriptors</a>, as well as optical flow for short term tracking. Line detection uses <a href="http://www.ipol.im/pub/art/2012/gjmr-lsd/">LSD - Line Segment Detector</a>. The primitive detection is based on <a href="https://arxiv.org/pdf/1803.02380.pdf">Fast Cylinder and Plane Extraction from Depth Cameras for Visual Odometry</a>.</p>
<p>For now, the program can do basic visual odometry with local mapping, and do not uses loop closures. The program runs above real time (200 - 400 FPS), the feature extraction process being the most time consumming part (50%).</p>
<h1><a class="anchor" id="autotoc_md2"></a>
Examples</h1>
<p>Some examples are provided, but their datasets must be downloaded separatly. A configuration file must be provided for every example, based on the model given in <b>configuration_example.yaml</b>. This configuration file must be placed in the <b>data/my_dataset</b> folder.</p>
<ul>
<li><b>CAPE</b> : The CAPE dataset contains RGB-D (size: 640x480) sequences of low textured environments, that are challenging for point-based SLAMs. It contains planes and cylinders, initially to test the CAPE process. The groundtrut trajectory are given for some sequences.</li>
<li><b>TUM-RGBD</b> : The TUM RGBD datasets are sequences of raw RGB-D (size: 640x480), with accelerometers data and groundtruth trajectories. There is a lot of different videos, with different conditions (translation/rotations only, dynamic environments, low texture, ...). The RGB and depth images must be synchronised by the user. The example program given here synchronises them with a greedy method.</li>
</ul>
<h1><a class="anchor" id="autotoc_md3"></a>
packages</h1>
<div class="fragment"><div class="line">opencv</div>
<div class="line">Eigen</div>
<div class="line">boost</div>
<div class="line">pugixml</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md4"></a>
Build and Run</h1>
<div class="fragment"><div class="line">mkdir build &amp;&amp; cd build</div>
<div class="line">cmake ..</div>
<div class="line">make</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md5"></a>
Run the tests</h2>
<div class="fragment"><div class="line">./test_p3p</div>
<div class="line">./testPoseOptimization</div>
<div class="line">./testKalmanFiltering</div>
</div><!-- fragment --><p>When all the tests are validated, you can run the main SLAM algorithm</p>
<h2><a class="anchor" id="autotoc_md6"></a>
How to use</h2>
<p>Use the provided example programs with the dataset at your disposal. The dataset should be located next to the src folder, in a data folder. Ex: For TUM fr1_xyz, you should place it in data/TUM/fr1_xyz</p>
<p>Running this program will produce a map file (format is .xyz for now) at the location of the executable.</p>
<p>While the program is running, an OpenCV windows displays the current frame with the tracked features in it. Each feature is assigned to a random color, that will never change during the mapping process. A good tracking/SLAM process will keep the same feature's colors during the whole process.</p>
<p>The top bar displays the number of points in the local map, as well as the planes and their colors.</p>
<h3><a class="anchor" id="autotoc_md7"></a>
CAPE</h3>
<p>CAPE provides the yoga and tunnel dataset, composed of low textured environment with cylinder primitives </p><div class="fragment"><div class="line">./slam_CAPE tunnel</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md8"></a>
TUM</h3>
<p>TUM contains many sequences, but it's best to start with the fr1_xyz and fr1_rpy (respectivly pure translations and pure rotations). </p><div class="fragment"><div class="line">./slam_TUM fr1_xyz</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md9"></a>
Launch parameters</h3>
<div class="fragment"><div class="line">-h Display the help</div>
<div class="line">-d displays the staged features (not yet in local map)</div>
<div class="line">-f path to the file containing the data (depth, rgb, cam parameters)</div>
<div class="line">-c use cylinder detection </div>
<div class="line">-j Drop j frames between slam process</div>
<div class="line">-i index of starting frame (&gt; 0)</div>
<div class="line">-l compute line features</div>
<div class="line">-s Save the trajectory in an output file</div>
<div class="line">-r FPS limiter, to debug sequences in real time</div>
</div><!-- fragment --><p>Check memory errors </p><div class="fragment"><div class="line">valgrind --suppressions=/usr/share/opencv4/valgrind.supp --suppressions=/usr/share/opencv4/valgrind_3rdparty.supp ./slam_TUM desk</div>
</div><!-- fragment --><p>Most of the program's parameters are stored in the <a class="el" href="parameters_8cpp.html">parameters.cpp</a> file. They can be modified as needed, and basic checks are launched at startup to detect erroneous parameters. The provided configuration should work for most of the use cases.</p>
<p>The user can also choose to run the program with deterministic results, by activating the <code>MAKE_DETERMINISTIC</code> option in the CMakeList.txt file. The maping process will be a bit slower but the result will always be the same between two sequences, allowing for reproductibility and debugging.</p>
<h1><a class="anchor" id="autotoc_md10"></a>
Detailed process</h1>
<h2><a class="anchor" id="autotoc_md11"></a>
feature detection &amp; matching</h2>
<p>The system starts by splitting the depth image as a 2D connected graph, and run a primitive analysis on it. This analysis extract local planar features, that are merged into bigger planes. Using an iterative RANSAC algorithm, those planar features are fitted to cylinders if possible, providing two kind of high level features. Those high level features are matched on one frame to another by comparing their normals and Inter-Over-Union score.</p>
<p>The system also extract points and lines, to improve reliability of pose extraction. The lines are extracted using LSD. The feature points are tracked by optical flow and matched by their ORB descriptors when needed, and every N frames to prevent optical flow drift.</p>
<h2><a class="anchor" id="autotoc_md12"></a>
local map</h2>
<p>Every features are maintained in a local map, that keep tracks of the reliable local features. Those features parameters (positions, uncertainties, speed, ...) are updated if a map feature is matched, using an individual Kalman filter per feature (no bundle adjustment yet).</p>
<p>New features are maintained in a staged feature container, that behaves exactly like the local map, but as a lesser priority during the matching step.</p>
<h2><a class="anchor" id="autotoc_md13"></a>
pose optimization</h2>
<p>The final pose for the frame is computed using the local map to detected feature matches. Outliers are filtered out using a simple RANSAC, and the final pose is computed using Levenberg Marquardt algorithm, with custom weighting functions (based on per feature covariance).</p>
<p>The optimized pose is used to update the local map and decaying motion model, in case the features are lost for a moment.</p>
<p>The complete systems runs in real time, between 300FPS and 600FPS for images of 640x480 pixels (depth and RGB images).</p>
<h1><a class="anchor" id="autotoc_md14"></a>
To be implemented soon</h1>
<ul>
<li>Advanced camera parameter model</li>
<li>Point descriptors based on custom neural network</li>
<li>Usage of keyframe to separate local maps</li>
<li>Loop closing based on multiscale neural network</li>
<li>Use a graph between keyframe to represent the trajectory, and close the loop easily (using bundle adjustment)</li>
<li>Create a new separate trajectory when loosing track of features, and merge it to the main trajectory when possible</li>
</ul>
<h1><a class="anchor" id="autotoc_md15"></a>
References:</h1>
<h2><a class="anchor" id="autotoc_md16"></a>
Primitive extraction</h2>
<ul>
<li><a href="https://arxiv.org/pdf/1803.02380.pdf">Fast Cylinder and Plane Extraction from Depth Cameras for Visual Odometry</a></li>
<li><a href="http://www.cs.cmu.edu/~mmv/papers/11rssw-BiswasVeloso2.pdf">Fast Sampling Plane Filtering, Polygon Construction and Merging from Depth Images</a></li>
<li><a href="https://merl.com/publications/docs/TR2014-066.pdf">Fast Plane Extraction in Organized Point Clouds Using Agglomerative HierarchicalClustering</a></li>
<li><a href="https://www.researchgate.net/publication/328822338_Depth_image-based_plane_detection">Depth image-based plane detection</a></li>
</ul>
<h2><a class="anchor" id="autotoc_md17"></a>
Pose Optimization</h2>
<ul>
<li><a href="https://www.yumpu.com/en/document/read/30132260/using-quaternions-for-parametrizing-3-d-rotations-in-">Using Quaternions for Parametrizing 3D Rotations in Unconstrained Nonlinear Optimization</a> - 2001</li>
<li>[At All Costs: A Comparison of Robust Cost Functions for Camera Correspondence Outliers]() - 2015</li>
<li>[An Eï¬€icient Solution to the Homography-Based Relative Pose Problem With a Common Reference Direction]() - 2019</li>
<li>[A Quaternion-based Certifiably Optimal Solution to the Wahba Problem with Outliers]()</li>
<li>[A General and Adaptive Robust Loss Function]() - 2019</li>
</ul>
<h2><a class="anchor" id="autotoc_md18"></a>
Visual odometry</h2>
<ul>
<li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6165120/">LVT: Lightweight Visual Odometry for Autonomous Mobile Robots</a></li>
<li><a href="https://www.researchgate.net/publication/283806535_Dense_RGB-D_visual_odometry_using_inverse_depth">Dense RGB-D visual odometry using inverse depth</a></li>
<li><a href="http://epubs.surrey.ac.uk/846020/1/SPLODE.pdf">SPLODE: Semi-Probabilistic Point and Line Odometry with Depth Estimation from RGB-D Camera Motion</a></li>
<li><a href="http://mapir.isa.uma.es/rgomez/publications/iros16plsvo.pdf">PL-SVO: Semi-Direct Monocular Visual Odometry byCombining Points and Line Segments</a></li>
<li><a href="https://arxiv.org/pdf/1705.06516v1.pdf">Probabilistic Combination of Noisy Points andPlanes for RGB-D Odometry</a> and <a href="https://arxiv.org/pdf/1706.04034.pdf">Probabilistic RGB-D Odometry based on Points, Linesand Planes Under Depth Uncertainty</a> - 2017</li>
<li><a href="https://www.mdpi.com/1424-8220/20/10/2922/htm">Robust Stereo Visual Inertial Navigation System Based on Multi-Stage Outlier Removal in Dynamic Environments</a></li>
<li>[RP-VIO: Robust Plane-based Visual-Inertial Odometry for Dynamic Environments]() - 2021</li>
</ul>
<h2><a class="anchor" id="autotoc_md19"></a>
SLAM</h2>
<ul>
<li><a href="https://arxiv.org/pdf/1610.06475.pdf">ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras</a></li>
<li><a href="https://www.researchgate.net/publication/283273992_3D_SLAM_in_texture-less_environments_using_rank_order_statistics">3D SLAM in texture-less environments using rank order statistics</a></li>
<li><a href="http://www2.informatik.uni-freiburg.de/~endres/files/publications/felix-endres-phd-thesis.pdf">3D Mapping with an RGB-D Camera</a></li>
<li><a href="http://robotics.jacobs-university.de/publicationData/JFR-3D-PlaneSLAM.pdf">Online 3D SLAM by Registration of Large PlanarSurface Segments and Closed Form Pose-GraphRelaxation</a></li>
<li><a href="http://udel.edu/~yuyang/downloads/geneva_iros2018.pdf">LIPS: LiDAR-Inertial 3D Plane SLAM</a></li>
<li><a href="https://arxiv.org/pdf/1703.07334.pdf">Pop-up SLAM: Semantic Monocular Plane SLAM for Low-texture Environments</a></li>
<li><a href="https://arxiv.org/pdf/1809.03415.pdf">Monocular Object and Plane SLAM in Structured Environments</a></li>
<li><a href="https://www.mdpi.com/1424-8220/19/17/3795/htm">Point-Plane SLAM Using Supposed Planes for Indoor Environments</a></li>
<li>[Stereo Plane SLAM Based on Intersecting Lines]() - 2020 </li>
</ul>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.6
</small></address>
</body>
</html>
