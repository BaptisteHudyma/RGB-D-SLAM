My take on a SLAM, based on shape primitive recognition with a RGB-\/D camera. The system uses depth informations as a connected 2D graph to extract primitive shapes, and use them with 3D points and lines to estimate it\textquotesingle{}s position in space. A map is created with those points, lines and primitive shapes.

The primitive detection is based on \href{https://arxiv.org/pdf/1803.02380.pdf}{\texttt{ Fast Cylinder and Plane Extraction from Depth Cameras for Visua Odometry}}. See the Doxygen documentation on \href{https://baptistehudyma.github.io/RGB-D-SLAM/html/index.html}{\texttt{ Git\+Hub Pages}}.

For now, we are at the state of visual odometry with local mapping.\hypertarget{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md1}{}\doxysection{Architectural description}\label{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md1}
\hypertarget{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md2}{}\doxysubsection{Sources}\label{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md2}
The program source files are organized as so\+:
\begin{DoxyItemize}
\item {\bfseries{features}}\+: Handles the feature detection and matching.
\begin{DoxyItemize}
\item {\bfseries{keypoints}}\+: detect and match keypoints, using their descriptors and short term optical flow
\item {\bfseries{primitives}}\+: Handle the detection and tracking of planes and cylinders, based on a connected graph technique described in \href{https://arxiv.org/pdf/1803.02380.pdf}{\texttt{ Fast Cylinder and Plane Extraction from Depth Cameras for Visua Odometry}}.
\end{DoxyItemize}
\item {\bfseries{map\+\_\+management}} The \char`\"{}\+Mapping\char`\"{} part of SLAM. It\textquotesingle{}s a local map implementation, with no loop closures
\item {\bfseries{outputs}} All the outputs that this program produces are stored here EXCEPT the images, that are handled by their classes (the map display is handled by the map class)
\item {\bfseries{pose\+\_\+optimization}} Contains all the pose optimization functions. The optimization process uses a Levenberg-\/\+Marquardt algorithm for frame to frame optimization
\item {\bfseries{tracking}} The tracking functions, as Kalman filters and observers motion model
\item {\bfseries{utils}} Math and cameras utils functions. Also contains the main types (eg\+: Pose)
\end{DoxyItemize}

A folder {\bfseries{third\+\_\+party}} for third party algorithms. This code is not mine, and source is given if available.\hypertarget{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md3}{}\doxysubsection{Examples}\label{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md3}
Some examples are given, but their dataset are not given. A configuration file must be given for every example, based on the model given in {\bfseries{configuration\+\_\+example.\+yaml}}. This configuration file must be placed in the {\bfseries{data/my\+\_\+dataset}} folder.


\begin{DoxyItemize}
\item {\bfseries{CAPE}} \+: The CAPE dataset contains RGB-\/D (size\+: 640x480) sequences of low textured environments, that are challenging for point-\/based SLAMs. It contains planes and cylinders, initially to test the CAPE process. The groundtrut trajectory are given for some sequences.
\item {\bfseries{freiburg-\/\+Berkeley}} \+: The Freiburg-\/\+Berkeley datasets are sequences of raw RGB-\/D (size\+: 640x480), with accelerometers data and groundtruth trajectories. Their is a lot of different videos, with different conditions (translation/rotations only, dynamic environments, ...)
\end{DoxyItemize}\hypertarget{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md4}{}\doxysubsection{packages}\label{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md4}

\begin{DoxyCode}{0}
\DoxyCodeLine{opencv}
\DoxyCodeLine{Eigen}
\DoxyCodeLine{g2o}
\DoxyCodeLine{pugixml}

\end{DoxyCode}
\hypertarget{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md5}{}\doxysubsection{Build and Run}\label{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md5}

\begin{DoxyCode}{0}
\DoxyCodeLine{mkdir build \&\& cd build}
\DoxyCodeLine{cmake ..}
\DoxyCodeLine{make}

\end{DoxyCode}
\hypertarget{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md6}{}\doxysubsubsection{Run the tests}\label{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md6}

\begin{DoxyCode}{0}
\DoxyCodeLine{./test\_p3p}
\DoxyCodeLine{./testPoseOptimization}
\DoxyCodeLine{./testKalmanFiltering}

\end{DoxyCode}


When all the tests are validated, you can run the main SLAM algorithm\hypertarget{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md7}{}\doxysubsubsection{How to use}\label{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md7}

\begin{DoxyCode}{0}
\DoxyCodeLine{./slam\_freiburg1 desk}

\end{DoxyCode}
 Parameters 
\begin{DoxyCode}{0}
\DoxyCodeLine{-\/h Display the help}
\DoxyCodeLine{-\/d displays the staged features (not yet in local map)}
\DoxyCodeLine{-\/f path to the file containing the data (depth, rgb, cam parameters)}
\DoxyCodeLine{-\/c use cylinder detection }
\DoxyCodeLine{-\/j Drop j frames between slam process}
\DoxyCodeLine{-\/i index of starting frame (> 0)}
\DoxyCodeLine{-\/l compute line features}
\DoxyCodeLine{-\/s Save the trajectory in an output file}

\end{DoxyCode}


Check memory errors 
\begin{DoxyCode}{0}
\DoxyCodeLine{valgrind -\/-\/suppressions=/usr/share/opencv4/valgrind.supp -\/-\/suppressions=/usr/share/opencv4/valgrind\_3rdparty.supp ./slam\_freiburg1 desk}

\end{DoxyCode}
\hypertarget{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md8}{}\doxysubsection{Detailed process}\label{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md8}
\hypertarget{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md9}{}\doxysubsubsection{feature detection \& matching}\label{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md9}
The system starts by splitting the depth image as a 2D connected graph, and run a primitive analysis on it. This analysis extract local planar features, that are merged into bigger planes. Using an iterative RANSAC algorithm, those planar features are fitted to cylinders if possible, providing two kind of high level features. Those high level features are matched on one frame to another by comparing their normals and Inter-\/\+Over-\/\+Union score.

The system also extract points and lines, to improve reliability of pose extraction. The lines are extracted using LSD. The feature points are tracked by optical flow and matched by their BRIEF descriptors when needed, and every N frames to prevent optical flow drift.\hypertarget{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md10}{}\doxysubsubsection{local map}\label{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md10}
Every features are maintained in a local map, that keep tracks of the reliable local features. Those features parameters (positions, uncertainties, speed, ...) is updated if a map feature is matched, using an individual Kalman filter per feature (no bundle adjustment yet).

New features are maintained in a staged feature container, that behaves exactly like the local map, but as a lesser priority during the matching step.\hypertarget{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md11}{}\doxysubsubsection{pose optimization}\label{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md11}
The final pose for the frame is computed using the local map to detected feature matches. Outliers are filtered out using a simple RANSAC, and the final pose is computed using Levenberg Marquardt algorithm, with custom weighting functions.

The optimized pose is used to update the local map and motion model, in case the features are lost for a moment.

The complete systems runs in real time, between 30FPS and 60FPS for images of 640x480 pixels (depth and RGB images).\hypertarget{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md12}{}\doxysubsection{To be implemented soon}\label{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md12}

\begin{DoxyItemize}
\item Advanced camera parameter model
\item Point descriptors based on custom neural network
\item Usage of keyframe to separate local maps
\item Loop closing based on multiscale neural network
\item Use a graph between keyframe to represent the trajectory, and close the loop easily (using bundle adjustment)
\item Create a new separate trajectory when loosing track of features, and merge it to the main trajectory when possible
\end{DoxyItemize}\hypertarget{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md13}{}\doxysection{References\+:}\label{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md13}
\hypertarget{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md14}{}\doxysubsection{Primitive extraction}\label{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md14}

\begin{DoxyItemize}
\item \href{https://arxiv.org/pdf/1803.02380.pdf}{\texttt{ Fast Cylinder and Plane Extraction from Depth Cameras for Visual Odometry}}
\item \href{http://www.cs.cmu.edu/~mmv/papers/11rssw-BiswasVeloso2.pdf}{\texttt{ Fast Sampling Plane Filtering, Polygon Construction and Merging from Depth Images}}
\item \href{https://merl.com/publications/docs/TR2014-066.pdf}{\texttt{ Fast Plane Extraction in Organized Point Clouds Using Agglomerative Hierarchical\+Clustering}}
\item \href{https://www.researchgate.net/publication/328822338_Depth_image-based_plane_detection}{\texttt{ Depth image-\/based plane detection}}
\end{DoxyItemize}\hypertarget{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md15}{}\doxysubsection{Pose Optimization}\label{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md15}

\begin{DoxyItemize}
\item \href{https://www.yumpu.com/en/document/read/30132260/using-quaternions-for-parametrizing-3-d-rotations-in-}{\texttt{ Using Quaternions for Parametrizing 3D Rotations in Unconstrained Nonlinear Optimization}} -\/ 2001
\item \mbox{[}At All Costs\+: A Comparison of Robust Cost Functions for Camera Correspondence Outliers\mbox{]}() -\/ 2015
\item \mbox{[}An Eï¬€icient Solution to the Homography-\/\+Based Relative Pose Problem With a Common Reference Direction\mbox{]}() -\/ 2019
\item \mbox{[}A Quaternion-\/based Certifiably Optimal Solution to the Wahba Problem with Outliers\mbox{]}()
\item \mbox{[}A General and Adaptive Robust Loss Function\mbox{]}() -\/ 2019
\end{DoxyItemize}\hypertarget{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md16}{}\doxysubsection{Visual odometry}\label{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md16}

\begin{DoxyItemize}
\item \href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6165120/}{\texttt{ LVT\+: Lightweight Visual Odometry for Autonomous Mobile Robots}}
\item \href{https://www.researchgate.net/publication/283806535_Dense_RGB-D_visual_odometry_using_inverse_depth}{\texttt{ Dense RGB-\/D visual odometry using inverse depth}}
\item \href{http://epubs.surrey.ac.uk/846020/1/SPLODE.pdf}{\texttt{ SPLODE\+: Semi-\/\+Probabilistic Point and Line Odometry with Depth Estimation from RGB-\/D Camera Motion}}
\item \href{http://mapir.isa.uma.es/rgomez/publications/iros16plsvo.pdf}{\texttt{ PL-\/\+SVO\+: Semi-\/\+Direct Monocular Visual Odometry by\+Combining Points and Line Segments}}
\item \href{https://arxiv.org/pdf/1705.06516v1.pdf}{\texttt{ Probabilistic Combination of Noisy Points and\+Planes for RGB-\/D Odometry}} and \href{https://arxiv.org/pdf/1706.04034.pdf}{\texttt{ Probabilistic RGB-\/D Odometry based on Points, Linesand Planes Under Depth Uncertainty}} -\/ 2017
\item \href{https://www.mdpi.com/1424-8220/20/10/2922/htm}{\texttt{ Robust Stereo Visual Inertial Navigation System Based on Multi-\/\+Stage Outlier Removal in Dynamic Environments}}
\item \mbox{[}RP-\/\+VIO\+: Robust Plane-\/based Visual-\/\+Inertial Odometry for Dynamic Environments\mbox{]}() -\/ 2021
\end{DoxyItemize}\hypertarget{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md17}{}\doxysubsection{SLAM}\label{md__home_baptisteh__documents_code_robots__r_g_b__s_l_a_m__r_g_b__d__s_l_a_m__r_e_a_d_m_e_autotoc_md17}

\begin{DoxyItemize}
\item \href{https://arxiv.org/pdf/1610.06475.pdf}{\texttt{ ORB-\/\+SLAM2\+: an Open-\/\+Source SLAM System for Monocular, Stereo and RGB-\/D Cameras}}
\item \href{https://www.researchgate.net/publication/283273992_3D_SLAM_in_texture-less_environments_using_rank_order_statistics}{\texttt{ 3D SLAM in texture-\/less environments using rank order statistics}}
\item \href{http://www2.informatik.uni-freiburg.de/~endres/files/publications/felix-endres-phd-thesis.pdf}{\texttt{ 3D Mapping with an RGB-\/D Camera}}
\item \href{http://robotics.jacobs-university.de/publicationData/JFR-3D-PlaneSLAM.pdf}{\texttt{ Online 3D SLAM by Registration of Large Planar\+Surface Segments and Closed Form Pose-\/\+Graph\+Relaxation}}
\item \href{http://udel.edu/~yuyang/downloads/geneva_iros2018.pdf}{\texttt{ LIPS\+: Li\+DAR-\/\+Inertial 3D Plane SLAM}}
\item \href{https://arxiv.org/pdf/1703.07334.pdf}{\texttt{ Pop-\/up SLAM\+: Semantic Monocular Plane SLAM for Low-\/texture Environments}}
\item \href{https://arxiv.org/pdf/1809.03415.pdf}{\texttt{ Monocular Object and Plane SLAM in Structured Environments}}
\item \href{https://www.mdpi.com/1424-8220/19/17/3795/htm}{\texttt{ Point-\/\+Plane SLAM Using Supposed Planes for Indoor Environments}}
\item \mbox{[}Stereo Plane SLAM Based on Intersecting Lines\mbox{]}() -\/ 2020 
\end{DoxyItemize}